{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dc9de5b3",
      "metadata": {
        "id": "dc9de5b3"
      },
      "source": [
        "\n",
        "# 00 ‚Äì Data Collection & Preparation for DietCheck\n",
        "\n",
        "**Course:** CS6120 ‚Äì Natural Language Processing  \n",
        "**Project:** DietCheck ‚Äì NLP System for Dietary Claim Verification  \n",
        "**Notebook:** `00` ‚Äì Core data preparation, numeric labels for Task 1, and claim subsets for Task 2.\n",
        "\n",
        "This notebook does the following:\n",
        "\n",
        "1. Loads the **core product table** (`products.csv`) for DietCheck.\n",
        "2. Computes **per-serving nutrition features** and **Task 1 dietary labels**:\n",
        "   - `keto_compliant`, `high_protein`, `low_sodium`, `low_fat`  \n",
        "     (using the FDA-style thresholds in the research plan).\n",
        "3. Creates **train/validation/test splits** with label-combination awareness.\n",
        "4. Extracts a **small, high-precision set of claim-like strings** from `products.csv`\n",
        "   for **Task 2 manual annotation** ‚Üí `candidate_claims_task2.csv`.\n",
        "5. Builds a **claim-rich subset from OpenFoodFacts via HuggingFace** using\n",
        "   `labels_tags` ‚Üí `openfoodfacts_claims_subset.csv` for additional Task 2 data.\n",
        "\n",
        "You should run this notebook top-to-bottom in a Colab or local environment with internet access\n",
        "(for the HuggingFace step).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Cell 1: Imports, paths, and logging\n",
        "# ======================================================================\n",
        "\n",
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import logging\n",
        "\n",
        "# Configure basic logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Data directory\n",
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"DATA_DIR set to: {DATA_DIR.resolve()}\")\n",
        "print(f\"Random seed set to: {RANDOM_SEED}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvQ8Cs2Rp3_L",
        "outputId": "b8fec12f-5f20-405a-e8b6-dec2b63ce4a6"
      },
      "id": "xvQ8Cs2Rp3_L",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA_DIR set to: /content/data\n",
            "Random seed set to: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Cell 2: Load products.csv\n",
        "# ======================================================================\n",
        "\n",
        "products_path = DATA_DIR / \"products.csv\"\n",
        "\n",
        "if not products_path.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Expected {products_path} to exist.\\n\"\n",
        "        \"Please copy your DietCheck products table to data/products.csv and re-run.\"\n",
        "    )\n",
        "\n",
        "df = pd.read_csv(products_path)\n",
        "print(f\"‚úì Loaded products.csv with shape: {df.shape}\")\n",
        "print(f\"‚úì Columns: {list(df.columns)}\")\n",
        "print(f\"‚úì Product count: {len(df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBXVAponp7V_",
        "outputId": "9772f00b-a6e7-478c-a410-265a800d893a"
      },
      "id": "sBXVAponp7V_",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Loaded products.csv with shape: (279, 29)\n",
            "‚úì Columns: ['product_id', 'name', 'brand', 'category', 'ingredients', 'serving_size_g', 'energy_100g', 'fat_100g', 'saturated_fat_100g', 'carbs_100g', 'fiber_100g', 'sugars_100g', 'protein_100g', 'sodium_100g', 'net_carbs_100g', 'energy_per_serving', 'fat_per_serving', 'saturated_fat_per_serving', 'carbs_per_serving', 'fiber_per_serving', 'sugars_per_serving', 'protein_per_serving', 'sodium_per_serving', 'net_carbs_per_serving', 'keto_compliant', 'high_protein', 'low_sodium', 'low_fat', 'label_combination']\n",
            "‚úì Product count: 279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Cell 3: Compute per-serving nutrition and Task 1 labels\n",
        "# ======================================================================\n",
        "\n",
        "print(\"\\n‚û§ Computing per-serving nutrition and Task 1 dietary labels...\\n\")\n",
        "\n",
        "# Check if per-serving columns already exist\n",
        "serving_cols = [\"energy_per_serving\", \"fat_per_serving\", \"protein_per_serving\",\n",
        "                \"sodium_per_serving\", \"carbs_per_serving\", \"fiber_per_serving\",\n",
        "                \"saturated_fat_per_serving\", \"sugars_per_serving\", \"net_carbs_per_serving\"]\n",
        "\n",
        "if all(col in df.columns for col in serving_cols):\n",
        "    print(\"‚úì Per-serving columns already exist, skipping computation\")\n",
        "else:\n",
        "    print(\"Computing per-serving values from 100g data...\")\n",
        "\n",
        "    # Compute per-serving values\n",
        "    df[\"energy_per_serving\"] = df[\"energy_100g\"] * df[\"serving_size_g\"] / 100.0\n",
        "    df[\"fat_per_serving\"] = df[\"fat_100g\"] * df[\"serving_size_g\"] / 100.0\n",
        "    df[\"saturated_fat_per_serving\"] = df[\"saturated_fat_100g\"] * df[\"serving_size_g\"] / 100.0\n",
        "    df[\"carbs_per_serving\"] = df[\"carbs_100g\"] * df[\"serving_size_g\"] / 100.0\n",
        "    df[\"fiber_per_serving\"] = df[\"fiber_100g\"] * df[\"serving_size_g\"] / 100.0\n",
        "    df[\"sugars_per_serving\"] = df[\"sugars_100g\"] * df[\"serving_size_g\"] / 100.0\n",
        "    df[\"protein_per_serving\"] = df[\"protein_100g\"] * df[\"serving_size_g\"] / 100.0\n",
        "    df[\"sodium_per_serving\"] = df[\"sodium_100g\"] * df[\"serving_size_g\"] / 100.0\n",
        "\n",
        "    # Net carbs\n",
        "    df[\"net_carbs_per_serving\"] = (\n",
        "        df[\"carbs_per_serving\"] - df[\"fiber_per_serving\"]\n",
        "    ).fillna(df[\"carbs_per_serving\"])\n",
        "\n",
        "    print(\"‚úì Per-serving values computed\")\n",
        "\n",
        "# Check if Task 1 labels already exist\n",
        "label_cols = [\"keto_compliant\", \"high_protein\", \"low_sodium\", \"low_fat\"]\n",
        "\n",
        "if all(col in df.columns for col in label_cols):\n",
        "    print(\"‚úì Task 1 labels already exist, skipping computation\")\n",
        "else:\n",
        "    print(\"Computing Task 1 dietary classification labels...\")\n",
        "\n",
        "    # FDA thresholds (adjusted for keto as per research plan)\n",
        "    KETO_NET_CARBS_THRESHOLD = 40.0  # Adjusted from 5g as documented\n",
        "    HIGH_PROTEIN_THRESHOLD = 10.0     # grams (20% DV)\n",
        "    LOW_SODIUM_THRESHOLD = 140.0      # mg\n",
        "    LOW_FAT_THRESHOLD = 3.0           # grams\n",
        "\n",
        "    df[\"keto_compliant\"] = (df[\"net_carbs_per_serving\"] <= KETO_NET_CARBS_THRESHOLD).fillna(False)\n",
        "    df[\"high_protein\"] = (df[\"protein_per_serving\"] >= HIGH_PROTEIN_THRESHOLD).fillna(False)\n",
        "    df[\"low_sodium\"] = (df[\"sodium_per_serving\"] <= LOW_SODIUM_THRESHOLD).fillna(False)\n",
        "    df[\"low_fat\"] = (df[\"fat_per_serving\"] <= LOW_FAT_THRESHOLD).fillna(False)\n",
        "\n",
        "    print(\"‚úì Task 1 labels computed\")\n",
        "\n",
        "# Create label combination for stratification\n",
        "df[\"label_combination\"] = (\n",
        "    df[\"keto_compliant\"].astype(str) + \"_\" +\n",
        "    df[\"high_protein\"].astype(str) + \"_\" +\n",
        "    df[\"low_sodium\"].astype(str) + \"_\" +\n",
        "    df[\"low_fat\"].astype(str)\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Total products: {len(df)}\")\n",
        "print(f\"‚úì Unique label combinations: {df['label_combination'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44U5IHpDp9Ah",
        "outputId": "0eb04bc2-adeb-46da-f442-5e5d20ec869e"
      },
      "id": "44U5IHpDp9Ah",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚û§ Computing per-serving nutrition and Task 1 dietary labels...\n",
            "\n",
            "‚úì Per-serving columns already exist, skipping computation\n",
            "‚úì Task 1 labels already exist, skipping computation\n",
            "\n",
            "‚úì Total products: 279\n",
            "‚úì Unique label combinations: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Cell 4: Data quality and stratification report\n",
        "# ======================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DATA QUALITY REPORT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Overall counts\n",
        "print(f\"\\nüìä OVERALL STATISTICS\")\n",
        "print(f\"  ‚Ä¢ Total products: {len(df)}\")\n",
        "print(f\"  ‚Ä¢ Products with ingredients: {df['ingredients'].notna().sum()}\")\n",
        "print(f\"  ‚Ä¢ Products with complete nutrition: {df[['fat_per_serving', 'protein_per_serving', 'sodium_per_serving', 'carbs_per_serving']].notna().all(axis=1).sum()}\")\n",
        "\n",
        "# Task 1 label distribution\n",
        "print(f\"\\nüè∑Ô∏è  TASK 1 LABEL DISTRIBUTION\")\n",
        "for label in [\"keto_compliant\", \"high_protein\", \"low_sodium\", \"low_fat\"]:\n",
        "    count = df[label].sum()\n",
        "    pct = (count / len(df) * 100) if len(df) > 0 else 0\n",
        "    print(f\"  ‚Ä¢ {label}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "# Category distribution (if exists)\n",
        "if \"category\" in df.columns:\n",
        "    print(f\"\\nüì¶ CATEGORY DISTRIBUTION (Top 8)\")\n",
        "    print(df[\"category\"].value_counts().head(8))\n",
        "\n",
        "# Label combinations\n",
        "print(f\"\\nüîÄ LABEL COMBINATIONS (Top 10)\")\n",
        "print(df[\"label_combination\"].value_counts().head(10))\n",
        "\n",
        "# Nutrient completeness\n",
        "print(f\"\\nüß™ NUTRIENT DATA COMPLETENESS\")\n",
        "nutrient_cols = [\"fat_per_serving\", \"protein_per_serving\", \"sodium_per_serving\",\n",
        "                 \"carbs_per_serving\", \"fiber_per_serving\", \"sugars_per_serving\"]\n",
        "for col in nutrient_cols:\n",
        "    if col in df.columns:\n",
        "        count = df[col].notna().sum()\n",
        "        pct = (count / len(df) * 100) if len(df) > 0 else 0\n",
        "        print(f\"  ‚Ä¢ {col}: {count}/{len(df)} ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxVnLaDwp_MW",
        "outputId": "023b12f8-1a4f-4caf-bbc5-5cddb72bed4b"
      },
      "id": "pxVnLaDwp_MW",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "DATA QUALITY REPORT\n",
            "======================================================================\n",
            "\n",
            "üìä OVERALL STATISTICS\n",
            "  ‚Ä¢ Total products: 279\n",
            "  ‚Ä¢ Products with ingredients: 279\n",
            "  ‚Ä¢ Products with complete nutrition: 279\n",
            "\n",
            "üè∑Ô∏è  TASK 1 LABEL DISTRIBUTION\n",
            "  ‚Ä¢ keto_compliant: 90 (32.3%)\n",
            "  ‚Ä¢ high_protein: 105 (37.6%)\n",
            "  ‚Ä¢ low_sodium: 124 (44.4%)\n",
            "  ‚Ä¢ low_fat: 103 (36.9%)\n",
            "\n",
            "üì¶ CATEGORY DISTRIBUTION (Top 8)\n",
            "category\n",
            "en:plant-based-foods-and-beverages         130\n",
            "en:dairies                                  47\n",
            "en:beverages-and-beverages-preparations     29\n",
            "en:condiments                               23\n",
            "en:snacks                                   18\n",
            "en:seafood                                  10\n",
            "en:meals                                     6\n",
            "en:meats-and-their-products                  5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üîÄ LABEL COMBINATIONS (Top 10)\n",
            "label_combination\n",
            "0_1_0_0    68\n",
            "1_0_1_1    40\n",
            "0_0_0_0    30\n",
            "0_0_1_0    29\n",
            "0_0_1_1    21\n",
            "1_0_0_0    15\n",
            "0_0_0_1    14\n",
            "1_0_1_0    13\n",
            "0_1_1_0    12\n",
            "1_0_0_1    12\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üß™ NUTRIENT DATA COMPLETENESS\n",
            "  ‚Ä¢ fat_per_serving: 279/279 (100.0%)\n",
            "  ‚Ä¢ protein_per_serving: 279/279 (100.0%)\n",
            "  ‚Ä¢ sodium_per_serving: 279/279 (100.0%)\n",
            "  ‚Ä¢ carbs_per_serving: 279/279 (100.0%)\n",
            "  ‚Ä¢ fiber_per_serving: 279/279 (100.0%)\n",
            "  ‚Ä¢ sugars_per_serving: 279/279 (100.0%)\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Cell 5: Create train/validation/test splits\n",
        "# ======================================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"\\n‚û§ Creating train/validation/test splits...\\n\")\n",
        "\n",
        "# First split: train vs (val+test)\n",
        "df_train, df_temp = train_test_split(\n",
        "    df,\n",
        "    test_size=0.3,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=df[\"label_combination\"]\n",
        ")\n",
        "\n",
        "# Second split: val vs test\n",
        "df_val, df_test = train_test_split(\n",
        "    df_temp,\n",
        "    test_size=0.5,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=df_temp[\"label_combination\"]\n",
        ")\n",
        "\n",
        "print(f\"‚úì Train set: {len(df_train)} products ({len(df_train)/len(df)*100:.1f}%)\")\n",
        "print(f\"‚úì Validation set: {len(df_val)} products ({len(df_val)/len(df)*100:.1f}%)\")\n",
        "print(f\"‚úì Test set: {len(df_test)} products ({len(df_test)/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Save splits\n",
        "df_train.to_csv(DATA_DIR / \"train.csv\", index=False)\n",
        "df_val.to_csv(DATA_DIR / \"val.csv\", index=False)\n",
        "df_test.to_csv(DATA_DIR / \"test.csv\", index=False)\n",
        "\n",
        "print(f\"\\n‚úì Saved splits to:\")\n",
        "print(f\"  ‚Ä¢ {DATA_DIR / 'train.csv'}\")\n",
        "print(f\"  ‚Ä¢ {DATA_DIR / 'val.csv'}\")\n",
        "print(f\"  ‚Ä¢ {DATA_DIR / 'test.csv'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "FyFydUUfqBBy",
        "outputId": "764ed8f4-df18-44d8-9f5e-900859e1463b"
      },
      "id": "FyFydUUfqBBy",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚û§ Creating train/validation/test splits...\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-712407699.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# First split: train vs (val+test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m df_train, df_temp = train_test_split(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2870\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2872\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_common_namespace_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1907\u001b[0m         \"\"\"\n\u001b[1;32m   1908\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1909\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1910\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mclass_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2318\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2319\u001b[0m                 \u001b[0;34m\"The least populated class in y has only 1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2320\u001b[0m                 \u001b[0;34m\" member, which is too few. The minimum\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Cell 6: Candidate claim extraction (HELPER for manual Task 2 annotation)\n",
        "# ======================================================================\n",
        "# NOTE: This cell ONLY extracts candidate text snippets.\n",
        "# All Task 2 labels (claim_verifiable, claim_conflict, explanations)\n",
        "# must be manually annotated by humans.\n",
        "# ======================================================================\n",
        "\n",
        "import re\n",
        "\n",
        "print(\"\\n‚û§ Extracting claim-like strings for MANUAL Task 2 annotation\\n\")\n",
        "print(\"‚ö†Ô∏è  This is a HELPER step - no automatic labels are created.\")\n",
        "print(\"‚ö†Ô∏è  All claim_verifiable and claim_conflict labels must be done manually.\\n\")\n",
        "\n",
        "source_df = df_train.copy()\n",
        "print(f\"  ‚Æï Using TRAINING split: {len(source_df)} products\")\n",
        "\n",
        "TEXT_FIELDS = [f for f in [\"name\", \"category\", \"brand\"] if f in source_df.columns]\n",
        "print(f\"  ‚Æï Scanning text fields: {TEXT_FIELDS}\\n\")\n",
        "\n",
        "CLAIM_PATTERNS = {\n",
        "    \"low_sugar\": [\n",
        "        r\"\\bno\\s+added\\s+sugar\\b\",\n",
        "        r\"\\bwithout\\s+added\\s+sugar\\b\",\n",
        "        r\"\\bsugar[-\\s]?free\\b\",\n",
        "    ],\n",
        "    \"low_fat\": [\n",
        "        r\"\\blow[-\\s]?fat\\b\",\n",
        "        r\"\\b0\\s*%\\s*fat\\b\",\n",
        "        r\"\\bfat[-\\s]?free\\b\",\n",
        "    ],\n",
        "    \"high_protein\": [\n",
        "        r\"\\b(high|rich)\\s+in\\s+protein\\b\",\n",
        "        r\"\\bprotein[-\\s]?rich\\b\",\n",
        "        r\"\\bsource\\s+of\\s+protein\\b\",\n",
        "    ],\n",
        "    \"high_fiber\": [\n",
        "        r\"\\b(high|rich)\\s+in\\s+fib(re|er)s?\\b\",\n",
        "        r\"\\bsource\\s+of\\s+fib(re|er)s?\\b\",\n",
        "    ],\n",
        "    \"low_sodium\": [\n",
        "        r\"\\blow\\s+(salt|sodium)\\b\",\n",
        "        r\"\\breduced\\s+salt\\b\",\n",
        "        r\"\\breduced\\s+sodium\\b\",\n",
        "        r\"\\bno\\s+added\\s+salt\\b\",\n",
        "    ],\n",
        "    \"gluten_free\": [\n",
        "        r\"\\bgluten[-\\s]?free\\b\",\n",
        "    ],\n",
        "    \"lactose_free\": [\n",
        "        r\"\\blactose[-\\s]?free\\b\",\n",
        "    ],\n",
        "    \"keto\": [\n",
        "        r\"\\bketo(?:genic)?\\b\",\n",
        "        r\"\\bketo[-\\s]?friendly\\b\",\n",
        "    ],\n",
        "    \"light\": [\n",
        "        r\"\\blight\\b\",\n",
        "        r\"\\blightly\\s+salted\\b\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "compiled_patterns = {\n",
        "    k: [re.compile(p, flags=re.IGNORECASE) for p in v]\n",
        "    for k, v in CLAIM_PATTERNS.items()\n",
        "}\n",
        "\n",
        "def extract_claims_from_text(pid, field_name, text, context_window=25):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "    candidates = []\n",
        "    for claim_type, regex_list in compiled_patterns.items():\n",
        "        for regex in regex_list:\n",
        "            for match in regex.finditer(text):\n",
        "                start, end = match.span()\n",
        "                left = max(0, start - context_window)\n",
        "                right = min(len(text), end + context_window)\n",
        "                snippet = text[left:right].strip()\n",
        "                candidates.append(\n",
        "                    {\n",
        "                        \"product_id\": pid,\n",
        "                        \"claim_text\": snippet,\n",
        "                        \"claim_type_hint\": claim_type,\n",
        "                        \"source_field\": field_name,\n",
        "                        \"full_text\": text,\n",
        "                    }\n",
        "                )\n",
        "    return candidates\n",
        "\n",
        "all_candidates = []\n",
        "for _, row in source_df.iterrows():\n",
        "    pid = row.get(\"product_id\", None)\n",
        "    for field in TEXT_FIELDS:\n",
        "        text = row.get(field, None)\n",
        "        all_candidates.extend(\n",
        "            extract_claims_from_text(pid, field, text, context_window=25)\n",
        "        )\n",
        "\n",
        "if not all_candidates:\n",
        "    print(\"‚ö†Ô∏è  No candidate claims found with current patterns.\")\n",
        "    candidates_df = pd.DataFrame(\n",
        "        columns=[\"product_id\", \"claim_text\", \"claim_type_hint\", \"source_field\", \"full_text\"]\n",
        "    )\n",
        "else:\n",
        "    candidates_df = pd.DataFrame(all_candidates)\n",
        "    candidates_df = candidates_df.drop_duplicates(\n",
        "        subset=[\"product_id\", \"claim_text\", \"claim_type_hint\", \"source_field\"]\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "print(f\"  ‚Æï Extracted {len(candidates_df)} claim-like strings\")\n",
        "\n",
        "if not candidates_df.empty:\n",
        "    print(\"\\n  ‚Æï Claim type hints (for manual review):\")\n",
        "    print(candidates_df[\"claim_type_hint\"].value_counts())\n",
        "    print(f\"\\n  ‚Æï Products with ‚â•1 claim snippet: {candidates_df['product_id'].nunique()}\")\n",
        "\n",
        "claims_path = DATA_DIR / \"candidate_claims_task2.csv\"\n",
        "candidates_df.to_csv(claims_path, index=False)\n",
        "print(f\"\\n‚úì Saved candidate claim strings to: {claims_path}\")\n",
        "print(\"  ‚Üí Use this CSV as a STARTING POINT for manual Task 2 annotation\")"
      ],
      "metadata": {
        "id": "nziftEq_qCvu"
      },
      "id": "nziftEq_qCvu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Cell 7: Load claim-rich OFF subset from HuggingFace\n",
        "# ======================================================================\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"\\n‚û§ Loading OpenFoodFacts claim-rich subset...\\n\")\n",
        "\n",
        "# Target claim tags from OFF\n",
        "TARGET_TAGS = [\n",
        "    \"en:no-gluten\",\n",
        "    \"en:gluten-free\",\n",
        "    \"en:vegan\",\n",
        "    \"en:vegetarian\",\n",
        "    \"en:organic\",\n",
        "    \"en:no-lactose\",\n",
        "    \"en:lactose-free\",\n",
        "    \"en:no-palm-oil\",\n",
        "    \"en:palm-oil-free\",\n",
        "    \"en:low-fat\",\n",
        "    \"en:low-sugar\",\n",
        "    \"en:no-added-sugar\",\n",
        "    \"en:high-protein\",\n",
        "    \"en:high-fiber\",\n",
        "]\n",
        "\n",
        "# Map OFF tags to our claim types\n",
        "CLAIM_LABEL_MAP = {\n",
        "    \"en:no-gluten\": \"gluten_free\",\n",
        "    \"en:gluten-free\": \"gluten_free\",\n",
        "    \"en:vegan\": \"vegan\",\n",
        "    \"en:vegetarian\": \"vegetarian\",\n",
        "    \"en:organic\": \"organic\",\n",
        "    \"en:no-lactose\": \"lactose_free\",\n",
        "    \"en:lactose-free\": \"lactose_free\",\n",
        "    \"en:no-palm-oil\": \"palm_oil_free\",\n",
        "    \"en:palm-oil-free\": \"palm_oil_free\",\n",
        "    \"en:low-fat\": \"low_fat\",\n",
        "    \"en:low-sugar\": \"low_sugar\",\n",
        "    \"en:no-added-sugar\": \"no_added_sugar\",\n",
        "    \"en:high-protein\": \"high_protein\",\n",
        "    \"en:high-fiber\": \"high_fiber\",\n",
        "}\n",
        "\n",
        "print(f\"Mapped labels: {len(CLAIM_LABEL_MAP)} unique tags\\n\")\n",
        "\n",
        "def extract_main_text(val):\n",
        "    \"\"\"Extract main text from multilingual dict or return string.\"\"\"\n",
        "    if isinstance(val, dict):\n",
        "        return val.get(\"en\", \"\") or val.get(\"en-us\", \"\") or \"\"\n",
        "    return str(val) if val else \"\"\n",
        "\n",
        "def extract_ingredients_text(val):\n",
        "    \"\"\"Extract ingredient text from multilingual dict.\"\"\"\n",
        "    if isinstance(val, dict):\n",
        "        return val.get(\"en\", \"\") or val.get(\"en-us\", \"\") or \"\"\n",
        "    return str(val) if val else \"\"\n",
        "\n",
        "def get_float(d, key):\n",
        "    \"\"\"Extract float from nutriments dict, return None if invalid.\"\"\"\n",
        "    if not isinstance(d, dict):\n",
        "        return None\n",
        "    val = d.get(key)\n",
        "    if val is None:\n",
        "        return None\n",
        "    try:\n",
        "        f = float(val)\n",
        "        return f if not (math.isnan(f) or math.isinf(f)) else None\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "def extract_nutriments(nutriments):\n",
        "    \"\"\"Extract key nutrients as floats (or None).\"\"\"\n",
        "    if not isinstance(nutriments, dict):\n",
        "        return {\n",
        "            \"energy_100g\": None,\n",
        "            \"fat_100g\": None,\n",
        "            \"saturated_fat_100g\": None,\n",
        "            \"carbs_100g\": None,\n",
        "            \"fiber_100g\": None,\n",
        "            \"sugars_100g\": None,\n",
        "            \"protein_100g\": None,\n",
        "            \"sodium_100g\": None,\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"energy_100g\": get_float(nutriments, \"energy-kcal_100g\"),\n",
        "        \"fat_100g\": get_float(nutriments, \"fat_100g\"),\n",
        "        \"saturated_fat_100g\": get_float(nutriments, \"saturated-fat_100g\"),\n",
        "        \"carbs_100g\": get_float(nutriments, \"carbohydrates_100g\"),\n",
        "        \"fiber_100g\": get_float(nutriments, \"fiber_100g\"),\n",
        "        \"sugars_100g\": get_float(nutriments, \"sugars_100g\"),\n",
        "        \"protein_100g\": get_float(nutriments, \"proteins_100g\"),\n",
        "        \"sodium_100g\": get_float(nutriments, \"sodium_100g\"),\n",
        "    }\n",
        "\n",
        "MAX_ROWS = 2000\n",
        "\n",
        "print(\"Loading OpenFoodFacts dataset (streaming from HuggingFace)...\")\n",
        "ds = load_dataset(\"openfoodfacts/product-database\", split=\"food\", streaming=True)\n",
        "\n",
        "rows = []\n",
        "seen_codes = set()\n",
        "n_scanned = 0\n",
        "\n",
        "for example in ds:\n",
        "    n_scanned += 1\n",
        "    labels_tags = example.get(\"labels_tags\") or []\n",
        "    labels_tags = [t for t in labels_tags if isinstance(t, str)]\n",
        "    matching_tags = [t for t in labels_tags if t in TARGET_TAGS]\n",
        "    if not matching_tags:\n",
        "        continue\n",
        "\n",
        "    code = example.get(\"code\")\n",
        "    if not code or code in seen_codes:\n",
        "        continue\n",
        "    seen_codes.add(code)\n",
        "\n",
        "    claim_types = []\n",
        "    for t in matching_tags:\n",
        "        mapped = CLAIM_LABEL_MAP.get(t)\n",
        "        if mapped:\n",
        "            claim_types.append(mapped)\n",
        "    if not claim_types:\n",
        "        continue\n",
        "\n",
        "    product_name = extract_main_text(example.get(\"product_name\"))\n",
        "    brand = (example.get(\"brands\") or \"\").strip()\n",
        "    categories = (example.get(\"categories\") or \"\").strip()\n",
        "    ingredients_text = extract_ingredients_text(example.get(\"ingredients_text\"))\n",
        "    labels_str = (example.get(\"labels\") or \"\").strip()\n",
        "    nutriments = extract_nutriments(example.get(\"nutriments\"))\n",
        "\n",
        "    row = {\n",
        "        \"product_id\": code,\n",
        "        \"name\": product_name,\n",
        "        \"brand\": brand,\n",
        "        \"category\": categories,\n",
        "        \"ingredients_text\": ingredients_text,\n",
        "        \"labels\": labels_str,\n",
        "        \"labels_tags\": \"|\".join(labels_tags),\n",
        "        \"claim_type_hint\": \";\".join(sorted(set(claim_types))),\n",
        "        \"source_field\": \"labels/labels_tags\",\n",
        "        \"full_text\": \" | \".join(\n",
        "            [x for x in [product_name, brand, categories, ingredients_text, labels_str] if x]\n",
        "        ),\n",
        "        **nutriments,\n",
        "    }\n",
        "    rows.append(row)\n",
        "\n",
        "    if len(rows) % 200 == 0:\n",
        "        print(f\"  ‚Æï Collected {len(rows)} claim-rich products (scanned {n_scanned})...\")\n",
        "\n",
        "    if len(rows) >= MAX_ROWS:\n",
        "        break\n",
        "\n",
        "print(f\"\\n‚úì Finished. Collected {len(rows)} claim-rich products (scanned {n_scanned} total rows).\")\n",
        "\n",
        "df_claims = pd.DataFrame(rows)\n",
        "hf_output_path = DATA_DIR / \"openfoodfacts_claims_subset.csv\"\n",
        "df_claims.to_csv(hf_output_path, index=False)\n",
        "print(f\"‚úì Saved claim-rich subset to: {hf_output_path}\")\n",
        "\n",
        "# Show summary\n",
        "print(f\"\\nüìä OFF Claim Subset Summary:\")\n",
        "print(f\"  ‚Ä¢ Total products: {len(df_claims)}\")\n",
        "print(f\"  ‚Ä¢ With nutrition data: {df_claims[['fat_100g', 'protein_100g', 'sodium_100g', 'carbs_100g']].notna().any(axis=1).sum()}\")\n",
        "print(f\"  ‚Ä¢ With ingredients: {df_claims['ingredients_text'].notna().sum()}\")"
      ],
      "metadata": {
        "id": "QTXnyJkQqHWM"
      },
      "id": "QTXnyJkQqHWM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Cell 7: Load claim-rich OFF subset from HuggingFace\n",
        "# ======================================================================\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"\\n‚û§ Loading OpenFoodFacts claim-rich subset...\\n\")\n",
        "\n",
        "# Target claim tags from OFF\n",
        "TARGET_TAGS = [\n",
        "    \"en:no-gluten\",\n",
        "    \"en:gluten-free\",\n",
        "    \"en:vegan\",\n",
        "    \"en:vegetarian\",\n",
        "    \"en:organic\",\n",
        "    \"en:no-lactose\",\n",
        "    \"en:lactose-free\",\n",
        "    \"en:no-palm-oil\",\n",
        "    \"en:palm-oil-free\",\n",
        "    \"en:low-fat\",\n",
        "    \"en:low-sugar\",\n",
        "    \"en:no-added-sugar\",\n",
        "    \"en:high-protein\",\n",
        "    \"en:high-fiber\",\n",
        "]\n",
        "\n",
        "# Map OFF tags to our claim types\n",
        "CLAIM_LABEL_MAP = {\n",
        "    \"en:no-gluten\": \"gluten_free\",\n",
        "    \"en:gluten-free\": \"gluten_free\",\n",
        "    \"en:vegan\": \"vegan\",\n",
        "    \"en:vegetarian\": \"vegetarian\",\n",
        "    \"en:organic\": \"organic\",\n",
        "    \"en:no-lactose\": \"lactose_free\",\n",
        "    \"en:lactose-free\": \"lactose_free\",\n",
        "    \"en:no-palm-oil\": \"palm_oil_free\",\n",
        "    \"en:palm-oil-free\": \"palm_oil_free\",\n",
        "    \"en:low-fat\": \"low_fat\",\n",
        "    \"en:low-sugar\": \"low_sugar\",\n",
        "    \"en:no-added-sugar\": \"no_added_sugar\",\n",
        "    \"en:high-protein\": \"high_protein\",\n",
        "    \"en:high-fiber\": \"high_fiber\",\n",
        "}\n",
        "\n",
        "print(f\"Mapped labels: {len(CLAIM_LABEL_MAP)} unique tags\\n\")\n",
        "\n",
        "def extract_main_text(val):\n",
        "    \"\"\"Extract main text from multilingual dict or return string.\"\"\"\n",
        "    if isinstance(val, dict):\n",
        "        return val.get(\"en\", \"\") or val.get(\"en-us\", \"\") or \"\"\n",
        "    return str(val) if val else \"\"\n",
        "\n",
        "def extract_ingredients_text(val):\n",
        "    \"\"\"Extract ingredient text from multilingual dict.\"\"\"\n",
        "    if isinstance(val, dict):\n",
        "        return val.get(\"en\", \"\") or val.get(\"en-us\", \"\") or \"\"\n",
        "    return str(val) if val else \"\"\n",
        "\n",
        "def get_float(d, key):\n",
        "    \"\"\"Extract float from nutriments dict, return None if invalid.\"\"\"\n",
        "    if not isinstance(d, dict):\n",
        "        return None\n",
        "    val = d.get(key)\n",
        "    if val is None:\n",
        "        return None\n",
        "    try:\n",
        "        f = float(val)\n",
        "        return f if not (math.isnan(f) or math.isinf(f)) else None\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "def extract_nutriments(nutriments):\n",
        "    \"\"\"Extract key nutrients as floats (or None).\"\"\"\n",
        "    if not isinstance(nutriments, dict):\n",
        "        return {\n",
        "            \"energy_100g\": None,\n",
        "            \"fat_100g\": None,\n",
        "            \"saturated_fat_100g\": None,\n",
        "            \"carbs_100g\": None,\n",
        "            \"fiber_100g\": None,\n",
        "            \"sugars_100g\": None,\n",
        "            \"protein_100g\": None,\n",
        "            \"sodium_100g\": None,\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"energy_100g\": get_float(nutriments, \"energy-kcal_100g\"),\n",
        "        \"fat_100g\": get_float(nutriments, \"fat_100g\"),\n",
        "        \"saturated_fat_100g\": get_float(nutriments, \"saturated-fat_100g\"),\n",
        "        \"carbs_100g\": get_float(nutriments, \"carbohydrates_100g\"),\n",
        "        \"fiber_100g\": get_float(nutriments, \"fiber_100g\"),\n",
        "        \"sugars_100g\": get_float(nutriments, \"sugars_100g\"),\n",
        "        \"protein_100g\": get_float(nutriments, \"proteins_100g\"),\n",
        "        \"sodium_100g\": get_float(nutriments, \"sodium_100g\"),\n",
        "    }\n",
        "\n",
        "MAX_ROWS = 2000\n",
        "\n",
        "print(\"Loading OpenFoodFacts dataset (streaming from HuggingFace)...\")\n",
        "ds = load_dataset(\"openfoodfacts/product-database\", split=\"food\", streaming=True)\n",
        "\n",
        "rows = []\n",
        "seen_codes = set()\n",
        "n_scanned = 0\n",
        "\n",
        "for example in ds:\n",
        "    n_scanned += 1\n",
        "    labels_tags = example.get(\"labels_tags\") or []\n",
        "    labels_tags = [t for t in labels_tags if isinstance(t, str)]\n",
        "    matching_tags = [t for t in labels_tags if t in TARGET_TAGS]\n",
        "    if not matching_tags:\n",
        "        continue\n",
        "\n",
        "    code = example.get(\"code\")\n",
        "    if not code or code in seen_codes:\n",
        "        continue\n",
        "    seen_codes.add(code)\n",
        "\n",
        "    claim_types = []\n",
        "    for t in matching_tags:\n",
        "        mapped = CLAIM_LABEL_MAP.get(t)\n",
        "        if mapped:\n",
        "            claim_types.append(mapped)\n",
        "    if not claim_types:\n",
        "        continue\n",
        "\n",
        "    product_name = extract_main_text(example.get(\"product_name\"))\n",
        "    brand = (example.get(\"brands\") or \"\").strip()\n",
        "    categories = (example.get(\"categories\") or \"\").strip()\n",
        "    ingredients_text = extract_ingredients_text(example.get(\"ingredients_text\"))\n",
        "    labels_str = (example.get(\"labels\") or \"\").strip()\n",
        "    nutriments = extract_nutriments(example.get(\"nutriments\"))\n",
        "\n",
        "    row = {\n",
        "        \"product_id\": code,\n",
        "        \"name\": product_name,\n",
        "        \"brand\": brand,\n",
        "        \"category\": categories,\n",
        "        \"ingredients_text\": ingredients_text,\n",
        "        \"labels\": labels_str,\n",
        "        \"labels_tags\": \"|\".join(labels_tags),\n",
        "        \"claim_type_hint\": \";\".join(sorted(set(claim_types))),\n",
        "        \"source_field\": \"labels/labels_tags\",\n",
        "        \"full_text\": \" | \".join(\n",
        "            [x for x in [product_name, brand, categories, ingredients_text, labels_str] if x]\n",
        "        ),\n",
        "        **nutriments,\n",
        "    }\n",
        "    rows.append(row)\n",
        "\n",
        "    if len(rows) % 200 == 0:\n",
        "        print(f\"  ‚Æï Collected {len(rows)} claim-rich products (scanned {n_scanned})...\")\n",
        "\n",
        "    if len(rows) >= MAX_ROWS:\n",
        "        break\n",
        "\n",
        "print(f\"\\n‚úì Finished. Collected {len(rows)} claim-rich products (scanned {n_scanned} total rows).\")\n",
        "\n",
        "df_claims = pd.DataFrame(rows)\n",
        "hf_output_path = DATA_DIR / \"openfoodfacts_claims_subset.csv\"\n",
        "df_claims.to_csv(hf_output_path, index=False)\n",
        "print(f\"‚úì Saved claim-rich subset to: {hf_output_path}\")\n",
        "\n",
        "# Show summary\n",
        "print(f\"\\nüìä OFF Claim Subset Summary:\")\n",
        "print(f\"  ‚Ä¢ Total products: {len(df_claims)}\")\n",
        "print(f\"  ‚Ä¢ With nutrition data: {df_claims[['fat_100g', 'protein_100g', 'sodium_100g', 'carbs_100g']].notna().any(axis=1).sum()}\")\n",
        "print(f\"  ‚Ä¢ With ingredients: {df_claims['ingredients_text'].notna().sum()}\")"
      ],
      "metadata": {
        "id": "rKHWo1lLqJok"
      },
      "id": "rKHWo1lLqJok",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Cell 9: Sample OFF products for manual annotation (FIXED)\n",
        "# ======================================================================\n",
        "\n",
        "print(\"\\n‚û§ Creating stratified OFF sample for manual annotation...\\n\")\n",
        "\n",
        "if len(df_filtered) == 0:\n",
        "    print(\"‚ö†Ô∏è  No products available for sampling.\")\n",
        "    print(\"   Skipping OFF sampling step.\")\n",
        "    sampled = pd.DataFrame()\n",
        "else:\n",
        "    # Create primary_claim from claim_type_hint\n",
        "    df_filtered[\"primary_claim\"] = (\n",
        "        df_filtered[\"claim_type_hint\"]\n",
        "        .fillna(\"\")\n",
        "        .str.split(\";\")\n",
        "        .str[0]\n",
        "        .str.strip()\n",
        "    )\n",
        "\n",
        "    # Remove rows where primary_claim is empty\n",
        "    df_filtered = df_filtered[df_filtered[\"primary_claim\"].ne(\"\")].copy()\n",
        "\n",
        "    print(f\"  ‚Æï Unique primary claims: {df_filtered['primary_claim'].nunique()}\")\n",
        "\n",
        "    # Target number of OFF products\n",
        "    TARGET_OFF = 120\n",
        "\n",
        "    per_label_target = max(1, TARGET_OFF // max(1, df_filtered[\"primary_claim\"].nunique()))\n",
        "\n",
        "    def _sample_group(g):\n",
        "        n = min(len(g), per_label_target)\n",
        "        return g.sample(n=n, random_state=RANDOM_SEED)\n",
        "\n",
        "    sampled = (\n",
        "        df_filtered\n",
        "        .groupby(\"primary_claim\", group_keys=False)\n",
        "        .apply(_sample_group)\n",
        "    )\n",
        "\n",
        "    # Top up if under target\n",
        "    if len(sampled) < TARGET_OFF:\n",
        "        remaining = df_filtered.drop(sampled.index)\n",
        "        extra_needed = TARGET_OFF - len(sampled)\n",
        "        if extra_needed > 0 and len(remaining) > 0:\n",
        "            extra = remaining.sample(\n",
        "                n=min(extra_needed, len(remaining)),\n",
        "                random_state=RANDOM_SEED,\n",
        "            )\n",
        "            sampled = pd.concat([sampled, extra], ignore_index=True)\n",
        "\n",
        "    sampled = sampled.reset_index(drop=True)\n",
        "\n",
        "    print(f\"\\n‚úì Final OFF sample size: {len(sampled)}\")\n",
        "    print(\"\\n  Claim distribution:\")\n",
        "    print(sampled[\"primary_claim\"].value_counts())\n",
        "\n",
        "    # Save sampled subset\n",
        "    off_sampled_path = DATA_DIR / \"openfoodfacts_off_sample_for_manual.csv\"\n",
        "    sampled.to_csv(off_sampled_path, index=False)\n",
        "    print(f\"\\n‚úì Saved OFF sample to: {off_sampled_path}\")"
      ],
      "metadata": {
        "id": "K63jsQhIqLTl"
      },
      "id": "K63jsQhIqLTl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Cell 10: Final summary and reproducibility check\n",
        "# ======================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"NOTEBOOK 00 COMPLETION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n‚úÖ COMPLETED TASKS:\")\n",
        "print(\"  1. Loaded base products.csv\")\n",
        "print(\"  2. Computed per-serving nutrition and Task 1 labels\")\n",
        "print(\"  3. Created stratified train/val/test splits\")\n",
        "print(\"  4. Generated candidate claims for manual Task 2 annotation\")\n",
        "print(\"  5. Loaded and filtered OpenFoodFacts claim-rich subset\")\n",
        "print(\"  6. Created stratified OFF sample for manual annotation\")\n",
        "\n",
        "print(\"\\nüìÅ OUTPUT FILES:\")\n",
        "files_created = [\n",
        "    \"train.csv\",\n",
        "    \"val.csv\",\n",
        "    \"test.csv\",\n",
        "    \"candidate_claims_task2.csv\",\n",
        "    \"openfoodfacts_claims_subset.csv\",\n",
        "    \"openfoodfacts_claims_filtered.csv\",\n",
        "    \"openfoodfacts_off_sample_for_manual.csv\"\n",
        "]\n",
        "\n",
        "for fname in files_created:\n",
        "    path = DATA_DIR / fname\n",
        "    if path.exists():\n",
        "        size = path.stat().st_size / 1024\n",
        "        print(f\"  ‚úì {fname} ({size:.1f} KB)\")\n",
        "    else:\n",
        "        print(f\"  ‚ö†Ô∏è  {fname} (NOT FOUND)\")\n",
        "\n",
        "print(\"\\nüìä CURRENT DATA STATUS:\")\n",
        "print(f\"  ‚Ä¢ Products in train/val/test: {len(df)}\")\n",
        "print(f\"  ‚Ä¢ Task 1 labels: ‚úì Complete\")\n",
        "print(f\"  ‚Ä¢ Task 2 claim candidates: {len(candidates_df)} snippets\")\n",
        "print(f\"  ‚Ä¢ Task 3 ready: ‚úì Ingredients available\")\n",
        "\n",
        "print(\"\\n‚è≠Ô∏è  NEXT STEPS FOR GRADE CONTRACT:\")\n",
        "print(\"\\n  FOR B GRADE (minimum 120 products):\")\n",
        "print(\"    ‚Ä¢ Current products: {len(df)}\")\n",
        "if len(df) >= 120:\n",
        "    print(\"    ‚úì You have enough products!\")\n",
        "else:\n",
        "    print(f\"    ‚ö†Ô∏è  Need {120 - len(df)} more products\")\n",
        "print(\"    ‚Ä¢ Manually annotate Task 1 on all products\")\n",
        "print(\"    ‚Ä¢ Manually annotate Task 2 on ‚â•50 claims\")\n",
        "print(\"    ‚Ä¢ Implement rule-based + TF-IDF baselines\")\n",
        "\n",
        "print(\"\\n  FOR B+ GRADE (minimum 180 products):\")\n",
        "if len(df) >= 180:\n",
        "    print(\"    ‚úì You have enough products!\")\n",
        "else:\n",
        "    print(f\"    ‚ö†Ô∏è  Need {180 - len(df)} more products\")\n",
        "print(\"    ‚Ä¢ Expand Task 2 to ‚â•120 claims with explanations\")\n",
        "print(\"    ‚Ä¢ Double-annotate ‚â•25 products (Cohen's kappa)\")\n",
        "print(\"    ‚Ä¢ Implement BERT baseline\")\n",
        "\n",
        "print(\"\\n  FOR A- GRADE (minimum 200 products):\")\n",
        "if len(df) >= 200:\n",
        "    print(\"    ‚úì You have enough products!\")\n",
        "else:\n",
        "    print(f\"    ‚ö†Ô∏è  Need {200 - len(df)} more products\")\n",
        "print(\"    ‚Ä¢ Expand Task 2 to ‚â•160 claims\")\n",
        "print(\"    ‚Ä¢ Start Task 3 BIO tagging on ‚â•80 products\")\n",
        "print(\"    ‚Ä¢ Implement multimodal model\")\n",
        "\n",
        "print(\"\\n  FOR A GRADE (minimum 200 products):\")\n",
        "if len(df) >= 200:\n",
        "    print(\"    ‚úì You have enough products!\")\n",
        "else:\n",
        "    print(f\"    ‚ö†Ô∏è  Need {200 - len(df)} more products\")\n",
        "print(\"    ‚Ä¢ Complete Task 3 on ‚â•120 products\")\n",
        "print(\"    ‚Ä¢ Implement claim-table model\")\n",
        "print(\"    ‚Ä¢ Perform slice-based analysis\")\n",
        "\n",
        "print(\"\\nüîí REPRODUCIBILITY:\")\n",
        "print(f\"  ‚Ä¢ Random seed: {RANDOM_SEED}\")\n",
        "print(\"  ‚Ä¢ All splits saved with stratification\")\n",
        "print(\"  ‚Ä¢ All intermediate files saved\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  IMPORTANT REMINDERS:\")\n",
        "print(\"  ‚Ä¢ candidate_claims_task2.csv is ONLY a helper\")\n",
        "print(\"  ‚Ä¢ ALL Task 2 labels must be manually annotated\")\n",
        "print(\"  ‚Ä¢ Task 3 BIO tagging must be manual\")\n",
        "print(\"  ‚Ä¢ Follow FDA thresholds for Task 1 verification\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Notebook 00 complete! Ready for manual annotation.\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "bMpVaXtVqUdA"
      },
      "id": "bMpVaXtVqUdA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}